{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSclir3olKp4"
   },
   "source": [
    "# Detecting depression in Tweets using TF-IDF and BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DPeA1dfh2FA"
   },
   "source": [
    "# Installing and importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "jcHvWQn9Nh5X",
    "outputId": "a93cf235-fd04-4ba3-f73f-dbd0d3f7750d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rizki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from math import log, sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SXLgxG5h7DI"
   },
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "uSPLlfPhNuW0",
    "outputId": "3fc43be8-69e3-422c-e002-40ebef64dc43"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  label\n",
       "0  just had a real good moment. i missssssssss hi...      0\n",
       "1         is reading manga  http://plurk.com/p/mzp1e      0\n",
       "2  @comeagainjen http://twitpic.com/2y2lx - http:...      0\n",
       "3  @lapcat Need to send 'em to my accountant tomo...      0\n",
       "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('Depression Sentiment Tweets.csv')\n",
    "tweets.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the data distribution between the labels first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "D8Isy2xHNygX",
    "outputId": "b48e3378-3aaf-4582-9e15-067510e10de7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    8000\n",
       "1    2314\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, The data is imbalanced. Let's try to balanced them using undersampling which is matching the majority class to have the same number of the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label distribution:\n",
      " label\n",
      "0    8000\n",
      "1    2314\n",
      "Name: count, dtype: int64\n",
      "Balanced label distribution:\n",
      " label\n",
      "1    2314\n",
      "0    2314\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@moodysgartner @ziyatong Exactly... that's my ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy Friday Eve everyone...  We're almost the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I can't diagnose him from any distance because...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In this episode, we address questions specific...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@viherrera omg vicky, you are so brave!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  label\n",
       "0  @moodysgartner @ziyatong Exactly... that's my ...      1\n",
       "1  Happy Friday Eve everyone...  We're almost the...      0\n",
       "2  I can't diagnose him from any distance because...      1\n",
       "3  In this episode, we address questions specific...      1\n",
       "4           @viherrera omg vicky, you are so brave!       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balance the dataset by undersampling the majority class\n",
    "counts = tweets['label'].value_counts()\n",
    "print('Original label distribution:\\n', counts)\n",
    "min_count = counts.min()\n",
    "\n",
    "tweets = pd.concat([\n",
    "    tweets[tweets['label'] == 0].sample(min_count, random_state=42),\n",
    "    tweets[tweets['label'] == 1].sample(min_count, random_state=42)\n",
    "], ignore_index=True)\n",
    "\n",
    "# Shuffle the balanced dataframe\n",
    "tweets = tweets.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print('Balanced label distribution:\\n', tweets['label'].value_counts())\n",
    "\n",
    "tweets.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's clean our data by creating a function to do Tokenization, Stemming, and Stop-word Removal.  \n",
    "- Tokenization splits raw texts into atomic units (tokens). The purpose is to converts free text into items a model can count.\n",
    "- Stemming reduces words to a common root from chopping affixes (e.g running -> run). This is very useful to reduce vocabulary size and aggregate counts for similar words.\n",
    "- Stop Word Removal removes very common words such as 'the', 'is', 'and' that typically carry little meaning. This could help to speeds up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_message(message, lower_case = True, stem = True, stop_words = True, gram = 2):\n",
    "    if lower_case:\n",
    "        message = message.lower()\n",
    "    words = word_tokenize(message)\n",
    "    words = [w for w in words if len(w) > 2]\n",
    "    if gram > 1:\n",
    "        w = []\n",
    "        for i in range(len(words) - gram + 1):\n",
    "            w += [' '.join(words[i:i + gram])]\n",
    "        return w\n",
    "    if stop_words:\n",
    "        sw = stopwords.words('english')\n",
    "        words = [word for word in words if word not in sw]\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64_4Pm5siBcY"
   },
   "source": [
    "# Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5WFF3VzmInP"
   },
   "source": [
    "As the number of data limited, I will use almost all the data for training (95%) and the rest for testing, hoping to get better generalization of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cZJmN8aOOoul"
   },
   "outputs": [],
   "source": [
    "trainIndex, testIndex = list(), list()\n",
    "for i in range(tweets.shape[0]):\n",
    "    if np.random.uniform(0, 1) < 0.95:\n",
    "        trainIndex += [i]\n",
    "    else:\n",
    "        testIndex += [i]\n",
    "trainData = tweets.iloc[trainIndex]\n",
    "testData = tweets.iloc[testIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the model using TF-IDF and (BOW) Classifiers.\n",
    "- TF-IDF (Term Frequency–Inverse Document Frequency) weight scores based on how important a word is to a document in a collection.\n",
    "- BOW (Bag of Words) is a simple representation counting occurrences of each token in a document. It produces a fixed-length vector per document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tYzcjULuPVyd"
   },
   "outputs": [],
   "source": [
    "class TweetClassifier(object):\n",
    "    def __init__(self, trainData, method = 'tf-idf'):\n",
    "        self.tweets, self.labels = trainData['message'], trainData['label']\n",
    "        self.method = method\n",
    "\n",
    "    def train(self):\n",
    "        self.calc_TF_and_IDF()\n",
    "        if self.method == 'tf-idf':\n",
    "            self.calc_TF_IDF()\n",
    "        else:\n",
    "            self.calc_prob()\n",
    "\n",
    "    def calc_prob(self):\n",
    "        self.prob_depressive = dict()\n",
    "        self.prob_positive = dict()\n",
    "        for word in self.tf_depressive:\n",
    "            self.prob_depressive[word] = (self.tf_depressive[word] + 1) / (self.depressive_words + \\\n",
    "                                                                len(list(self.tf_depressive.keys())))\n",
    "        for word in self.tf_positive:\n",
    "            self.prob_positive[word] = (self.tf_positive[word] + 1) / (self.positive_words + \\\n",
    "                                                                len(list(self.tf_positive.keys())))\n",
    "        self.prob_depressive_tweet, self.prob_positive_tweet = self.depressive_tweets / self.total_tweets, self.positive_tweets / self.total_tweets\n",
    "\n",
    "\n",
    "    def calc_TF_and_IDF(self):\n",
    "        noOfMessages = self.tweets.shape[0]\n",
    "        self.depressive_tweets, self.positive_tweets = self.labels.value_counts()[1], self.labels.value_counts()[0]\n",
    "        self.total_tweets = self.depressive_tweets + self.positive_tweets\n",
    "        self.depressive_words = 0\n",
    "        self.positive_words = 0\n",
    "        self.tf_depressive = dict()\n",
    "        self.tf_positive = dict()\n",
    "        self.idf_depressive = dict()\n",
    "        self.idf_positive = dict()\n",
    "        for i in range(noOfMessages):\n",
    "            message_processed = process_message(self.tweets.iloc[i])\n",
    "            count = list() #To keep track of whether the word has ocured in the message or not.\n",
    "                           #For IDF\n",
    "            for word in message_processed:\n",
    "                if self.labels.iloc[i]:\n",
    "                    self.tf_depressive[word] = self.tf_depressive.get(word, 0) + 1\n",
    "                    self.depressive_words += 1\n",
    "                else:\n",
    "                    self.tf_positive[word] = self.tf_positive.get(word, 0) + 1\n",
    "                    self.positive_words += 1\n",
    "                if word not in count:\n",
    "                    count += [word]\n",
    "            for word in count:\n",
    "                if self.labels.iloc[i]:\n",
    "                    self.idf_depressive[word] = self.idf_depressive.get(word, 0) + 1\n",
    "                else:\n",
    "                    self.idf_positive[word] = self.idf_positive.get(word, 0) + 1\n",
    "\n",
    "    def calc_TF_IDF(self):\n",
    "        self.prob_depressive = dict()\n",
    "        self.prob_positive = dict()\n",
    "        self.sum_tf_idf_depressive = 0\n",
    "        self.sum_tf_idf_positive = 0\n",
    "        for word in self.tf_depressive:\n",
    "            self.prob_depressive[word] = (self.tf_depressive[word]) * log((self.depressive_tweets + self.positive_tweets) \\\n",
    "                                                          / (self.idf_depressive[word] + self.idf_positive.get(word, 0)))\n",
    "            self.sum_tf_idf_depressive += self.prob_depressive[word]\n",
    "        for word in self.tf_depressive:\n",
    "            self.prob_depressive[word] = (self.prob_depressive[word] + 1) / (self.sum_tf_idf_depressive + len(list(self.prob_depressive.keys())))\n",
    "\n",
    "        for word in self.tf_positive:\n",
    "            self.prob_positive[word] = (self.tf_positive[word]) * log((self.depressive_tweets + self.positive_tweets) \\\n",
    "                                                          / (self.idf_depressive.get(word, 0) + self.idf_positive[word]))\n",
    "            self.sum_tf_idf_positive += self.prob_positive[word]\n",
    "        for word in self.tf_positive:\n",
    "            self.prob_positive[word] = (self.prob_positive[word] + 1) / (self.sum_tf_idf_positive + len(list(self.prob_positive.keys())))\n",
    "\n",
    "\n",
    "        self.prob_depressive_tweet, self.prob_positive_tweet = self.depressive_tweets / self.total_tweets, self.positive_tweets / self.total_tweets\n",
    "\n",
    "    def classify(self, processed_message):\n",
    "        pDepressive, pPositive = 0, 0\n",
    "        for word in processed_message:\n",
    "            if word in self.prob_depressive:\n",
    "                pDepressive += log(self.prob_depressive[word])\n",
    "            else:\n",
    "                if self.method == 'tf-idf':\n",
    "                    pDepressive -= log(self.sum_tf_idf_depressive + len(list(self.prob_depressive.keys())))\n",
    "                else:\n",
    "                    pDepressive -= log(self.depressive_words + len(list(self.prob_depressive.keys())))\n",
    "            if word in self.prob_positive:\n",
    "                pPositive += log(self.prob_positive[word])\n",
    "            else:\n",
    "                if self.method == 'tf-idf':\n",
    "                    pPositive -= log(self.sum_tf_idf_positive + len(list(self.prob_positive.keys())))\n",
    "                else:\n",
    "                    pPositive -= log(self.positive_words + len(list(self.prob_positive.keys())))\n",
    "            pDepressive += log(self.prob_depressive_tweet)\n",
    "            pPositive += log(self.prob_positive_tweet)\n",
    "        return pDepressive >= pPositive\n",
    "\n",
    "    def predict(self, testData):\n",
    "        result = dict()\n",
    "        for (i, message) in enumerate(testData):\n",
    "            processed_message = process_message(message)\n",
    "            result[i] = int(self.classify(processed_message))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "K0yrhh0xPm90"
   },
   "outputs": [],
   "source": [
    "def metrics(labels, predictions):\n",
    "    true_pos, true_neg, false_pos, false_neg = 0, 0, 0, 0\n",
    "    for i in range(len(labels)):\n",
    "        true_pos += int(labels.iloc[i] == 1 and predictions[i] == 1)\n",
    "        true_neg += int(labels.iloc[i] == 0 and predictions[i] == 0)\n",
    "        false_pos += int(labels.iloc[i] == 0 and predictions[i] == 1)\n",
    "        false_neg += int(labels.iloc[i] == 1 and predictions[i] == 0)\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    Fscore = 2 * precision * recall / (precision + recall)\n",
    "    accuracy = (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)\n",
    "\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F-score: \", Fscore)\n",
    "    print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "s_eBmu6tPrDN",
    "outputId": "3f76d478-9cb6-4842-9824-627e0682e05d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.971830985915493\n",
      "Recall:  0.6052631578947368\n",
      "F-score:  0.745945945945946\n",
      "Accuracy:  0.8016877637130801\n"
     ]
    }
   ],
   "source": [
    "sc_tf_idf = TweetClassifier(trainData, 'tf-idf')\n",
    "sc_tf_idf.train()\n",
    "preds_tf_idf = sc_tf_idf.predict(testData['message'])\n",
    "metrics(testData['label'], preds_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "Y6-fuPbRPzOL",
    "outputId": "9bc2b6a7-d362-442d-a519-4a512aa4da24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  1.0\n",
      "Recall:  0.3684210526315789\n",
      "F-score:  0.5384615384615384\n",
      "Accuracy:  0.6962025316455697\n"
     ]
    }
   ],
   "source": [
    "sc_bow = TweetClassifier(trainData, 'bow')\n",
    "sc_bow.train()\n",
    "preds_bow = sc_bow.predict(testData['message'])\n",
    "metrics(testData['label'], preds_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the F-Score, TF-IDF model performs better than BOW. This is because TF‑IDF weights terms by importance (term frequency × inverse document frequency). This downweights very common words and upweights words that are rare across the corpus but frequent in a document — so discriminative words get more influence. BOW (raw counts) treats all tokens equally, so frequent but uninformative tokens can dominate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
